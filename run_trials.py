# -*- coding: utf-8 -*-
"""run_trials.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YyHbpFAhej3_vUf_FC4slzUgPzsE0_Wk
"""

import numpy as np

def run_trials(agent,environment,mousedata):
  synthetic_choices = np.zeros(mousedata['nTrials'])
  synthetic_rewards = np.zeros(mousedata['nTrials'])
  synthetic_qs = np.zeros((mousedata['nTrials'],2))

  for t in range(mousedata['nTrials']):
    # Ask the agent for a choice
    choice, choice_prob = agent.get_choice()
    # Step the environment, get a reward
    reward = environment.next_trial(choice)
    # Let the agent know about the reward
    agent.update(choice = choice, reward = reward, session_index = mousedata['session_index'])
    
    # Record the choice, reward
    synthetic_choices[t] = choice
    synthetic_rewards[t] = reward
    synthetic_qs[t] = agent.q
    
    synthetic_choices = synthetic_choices.astype(int)
    synthetic_rewards = synthetic_rewards.astype(int)

  return synthetic_choices, synthetic_rewards, synthetic_qs

def run_trials_for_ideal_observer(agent, environment, mousedata):
  synthetic_choices = np.zeros(mousedata['nTrials'])
  synthetic_rewards = np.zeros(mousedata['nTrials'])
  synthetic_belief = np.zeros(mousedata['nTrials'])

  for t in range(mousedata['nTrials']):
    # Ask the agent for a choice
    choice, prob_choose_left = agent.get_choice()
    # Step the environment, get a reward
    reward = environment.next_trial(choice)
    # Let the agent know about the reward
    agent.update(choice = choice, reward = reward, session_index = mousedata['session_index'])
    
    # Record the choice, reward
    synthetic_choices[t] = choice
    synthetic_rewards[t] = reward
    synthetic_belief[t] = agent.belief_left_better
    
    synthetic_choices = synthetic_choices.astype(int)
    synthetic_rewards = synthetic_rewards.astype(int)

  return synthetic_choices, synthetic_rewards, synthetic_belief

def run_trials_for_mvt(agent,environment,mousedata):
# Empty variables to accumulate choices, rewards, block, qs
  synthetic_choices = np.zeros(mousedata['nTrials'])
  synthetic_rewards = np.zeros(mousedata['nTrials'])
  synthetic_local_reward = np.zeros(mousedata['nTrials'])

  for t in range(mousedata['nTrials']):
    # Ask the agent for a choice
    choice, prob_choose_left = agent.get_choice()
    # Step the environment, get a reward
    reward = environment.next_trial(choice)
    # Let the agent know about the reward
    agent.update(choice = choice, reward = reward, session_index = mousedata['session_index'])
    
    # Record the choice, reward
    synthetic_choices[t] = choice
    synthetic_rewards[t] = reward
    synthetic_local_reward[t] = agent.local_reward
    
    synthetic_choices = synthetic_choices.astype(int)
    synthetic_rewards = synthetic_rewards.astype(int)

  return synthetic_choices, synthetic_rewards, synthetic_local_reward