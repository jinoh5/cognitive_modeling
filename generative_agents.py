# -*- coding: utf-8 -*-
"""generative_agents.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rqfmtm3qvkhh5DNbfjXC1mHbCtWKoRSm
"""

from xml.etree.ElementTree import QName
import numpy as np

'''
Compilation of all the synthetic agents/models 
Left side is coded as 0, and right side is coded as 1. 

Quick explanation of the hyperparameters: 
- learning rate: It defines how fast the agent learns. 
- Decision noise: It works to make variability when making a decision.
- beta_bias: It works as a perservative agent. It is considered as a
           fixed bias towards the left or right port 

'''

class vanillaQ_agent: #1 - An agent implementing textbook Q-learning, with a softmax policy

  def __init__(
      self,
      learning_rate,
      decision_noise,
      beta_bias):
    self._learning_rate = learning_rate
    self._decision_noise = decision_noise
    self._beta_bias = beta_bias
    self.current_index = 0
    self.initialize_new_session()

  def initialize_new_session(self):
    # Internal value 
    self.q = 0.5 * np.ones(2)
  
  def softmax(self, x):
    # Make the choice probability between 0 and 1 
    softmax_x = np.exp(x) / np.sum(np.exp(x))
    return softmax_x

  def get_choice(self):
    self.q[0] = self.q[0] + self._beta_bias
    self.q[1] = self.q[1] - self._beta_bias
    # Draw choice probabilities according to the softmax
    choice_probs = self.softmax(self._decision_noise * self.q)
    # Select a choice according to the probabilities
    choice = np.random.binomial(1, choice_probs[1])
    return choice, choice_probs[1]

  def update(self, choice, reward, session_index):
    # Update the q-value of the chosen side towards the reward
    self.q[choice] = self.q[choice] * (1 - self._learning_rate) + self._learning_rate * reward

    self.current_index += 1

    # If the next trial is the beginning of a new session, reset beliefs 
    if self.current_index in session_index: 
      self.initialize_new_session()
  
  def reset_index(self):
    self.current_index = 0

class optimisticQ_agent: #2 

  def __init__(
      self,
      learning_rate_pos,
      learning_rate_neg,
      decision_noise,
      beta_bias):
    self._learning_rate_pos = learning_rate_pos
    self._learning_rate_neg = learning_rate_neg 
    self._decision_noise = decision_noise
    self._beta_bias = beta_bias
    self.current_index = 0
    self.initialize_new_session()

  def initialize_new_session(self):
    self.q = 0.5 * np.ones(2)

  def softmax(self, x):
    softmax_x = np.exp(x) / np.sum(np.exp(x))
    return softmax_x

  def get_choice(self):

    self.q[0] = self.q[0] + self._beta_bias
    self.q[1] = self.q[1] - self._beta_bias
    
    choice_probs = self.softmax(self._decision_noise * self.q)
    choice = np.random.binomial(1, choice_probs[1])
    return choice, choice_probs[1]

  def update(self, choice, reward, session_index):
    prediction_error = reward-self.q[choice]

    #optimistic
    if prediction_error > 0:
      self.q[choice] = self.q[choice]+self._learning_rate_pos * prediction_error
    else: 
      self.q[choice] = self.q[choice]+self._learning_rate_neg * prediction_error
    
    self.current_index += 1

    # If the next trial is the beginning of a new session, reset beliefs 
    if self.current_index in session_index: 
      self.initialize_new_session()

  def reset_index(self):
    self.current_index = 0

class generalized_local_matching_law_agent: #3 - A modified version from Sugrue et al., (2004) 

  def __init__(
      self,
      learning_rate,
      sensitivity,
      bias):   
    self._learning_rate = learning_rate
    self._sensitivity = sensitivity
    self._bias = bias
    self.current_index = 0
    self.initialize_new_session()

  def initialize_new_session(self):
    self.q = 0.5 * np.ones(2)

  def get_choice(self): 
    prob_choose_left = self._bias*((self.q[1]/(self.q[0]+self.q[1]))**self._sensitivity)
    # Select a choice according to the probabilities
    choice = np.random.binomial(1, prob_choose_left)
    return choice, prob_choose_left

  def update(self, choice, reward, session_index): 

    if choice==0 and reward==1: # chose left side and received reward 
      self.q[0] = (1-self._learning_rate)*self.q[0]+self._learning_rate
      self.q[1] = (1-self._learning_rate)*self.q[1]

    elif choice==1 and reward==1: #chose right side and received reward 
      self.q[0] = (1-self._learning_rate)*self.q[0]
      self.q[1] = (1-self._learning_rate)*self.q[1]+self._learning_rate

    elif reward == 0:
      self.q[0] = (1-self._learning_rate)*self.q[0]
      self.q[1] = (1-self._learning_rate)*self.q[1]
    
    else: 
      ValueError("Something is wrong with updating Q")
    
    self.current_index += 1

    # If the next trial is the beginning of a new session, reset beliefs 
    if self.current_index in session_index: 
      self.initialize_new_session()

  def reset_index(self):
    self.current_index = 0

class forgettingQ_agent: #4 - Barraclough, 2014
  
  def __init__(
      self,
      learning_rate,
      reward_strength,
      aversion_strength,
      beta_bias):
    self._learning_rate = learning_rate  
    self._reward_strength = reward_strength
    self._aversion_strength = aversion_strength
    self._beta_bias = beta_bias
    self.current_index = 0
    self.initialize_new_session()

  def initialize_new_session(self):
    self.q = 0.5 * np.ones(2)
  
  def softmax(self, x):
    softmax_x = np.exp(x) / np.sum(np.exp(x))
    return softmax_x

  def get_choice(self):
    self.q[0] = self.q[0] + self._beta_bias
    self.q[1] = self.q[1] - self._beta_bias
    choice_probs = self.softmax(self.q)
    choice = np.random.binomial(1, choice_probs[1])
    return choice, choice_probs[1]

  def update(self, choice, reward, session_index):

    if choice==0 and reward==1: # chose left side and received reward 
      self.q[0] = (1-self._learning_rate)*self.q[0]+self._learning_rate*self._reward_strength
      self.q[1] = (1-self._learning_rate)*self.q[1]

    elif choice==0 and reward==0: #chose left side but did not receive reward
      self.q[0] = (1-self._learning_rate)*self.q[0]-self._learning_rate*self._aversion_strength
      self.q[1] = (1-self._learning_rate)*self.q[1]

    elif choice==1 and reward==1: #chose right side and received reward
      self.q[0] = (1-self._learning_rate)*self.q[0]
      self.q[1] = (1-self._learning_rate)*self.q[1]+self._learning_rate*self._reward_strength

    elif choice==1 and reward==0: #chose right side but did not receive reward
      self.q[0] = (1-self._learning_rate)*self.q[0]
      self.q[1] = (1-self._learning_rate)*self.q[1]-self._learning_rate*self._aversion_strength

    else: 
      ValueError("Something is wrong with updating Q")
    
    self.current_index += 1

    # If the next trial is the beginning of a new session, reset beliefs 
    if self.current_index in session_index: 
      self.initialize_new_session()
  
  def reset_index(self):
    self.current_index = 0

class differential_forgettingQ_agent: #5 - Ito and Doya, 2009

  def __init__(
      self,
      learning_rate,
      forgetting_rate,
      reward_strength,
      aversion_strength,
      beta_bias): 
    self._learning_rate = learning_rate 
    self._forgetting_rate = forgetting_rate 
    self._reward_strength = reward_strength
    self._aversion_strength = aversion_strength
    self._beta_bias = beta_bias 
    self.current_index = 0
    self.initialize_new_session()
  
  def initialize_new_session(self):
    self.q = 0.5 * np.ones(2)  

  def softmax(self, x): 
    softmax_x = np.exp(x) / np.sum(np.exp(x))
    return softmax_x

  def get_choice(self):
    self.q[0] = self.q[0] + self._beta_bias
    self.q[1] = self.q[1] - self._beta_bias
    choice_probs = self.softmax(self.q)
    choice = np.random.binomial(1, choice_probs[1])
    return choice, choice_probs[1]

  def update(self, choice, reward, session_index):

    if choice==0 and reward==1: # chose left side and received reward 
      self.q[0] = (1-self._learning_rate)*self.q[0]+self._learning_rate*self._reward_strength
      self.q[1] = (1-self._forgetting_rate)*self.q[1]

    elif choice==0 and reward==0: #chose left side but did not receive reward
      self.q[0] = (1-self._learning_rate)*self.q[0]-self._learning_rate*self._aversion_strength
      self.q[1] = (1-self._forgetting_rate)*self.q[1]

    elif choice==1 and reward==1: #chose right side and received reward
      self.q[0] = (1-self._forgetting_rate)*self.q[0]
      self.q[1] = (1-self._learning_rate)*self.q[1]+self._learning_rate*self._reward_strength

    elif choice==1 and reward==0: #chose right side but did not receive reward
      self.q[0] = (1-self._forgetting_rate)*self.q[0]
      self.q[1] = (1-self._learning_rate)*self.q[1]-self._learning_rate*self._aversion_strength
      
    else: 
      ValueError("Something is wrong with updating Q")
    
    self.current_index += 1

    # If the next trial is the beginning of a new session, reset beliefs 
    if self.current_index in session_index: 
      self.initialize_new_session()
  
  def reset_index(self):
    self.current_index = 0

class habitsRL_agent: #6 - Simplified from the model from Miller et al. (2017)
  # Two-process mixture model, with a reward-seeking and a habitual component

  def __init__(
      self,
      alpha_rl,
      alpha_habit,
      beta_rl,
      beta_habit,
      beta_bias
  ):
    self._alpha_rl = alpha_rl
    self._alpha_habit = alpha_habit
    self._beta_rl = beta_rl
    self._beta_habit = beta_habit
    self._beta_bias = beta_bias
    self.current_index = 0
    self.initialize_new_session()

  def initialize_new_session(self):
    # Initialize Q and H
    self.q = 0.5
    self.h = 0

  def get_choice(self):
    # Compute choice internal variable
    decision_variable = self._beta_rl * self.q + self._beta_habit * self.h + self._beta_bias
    # Convert to a choice probability
    choice_prob = 1 / (1 + np.exp(-1*decision_variable))
    # Select a choice stochastically according to the probability
    choice = np.random.binomial(1, choice_prob)
    return choice, choice_prob

  def update(self, choice, reward, session_index):
    
    # Convert choice and reward from 0/1 to -1/+1
    choice_for_update = 2*choice - 1   
    reward_for_update = 2*reward - 1  
    
    # Check that the conversion worked
    assert choice_for_update == 1 or choice_for_update == -1
    assert reward_for_update == 1 or reward_for_update == -1

    # Update Q
    self.q = self.q * (1 - self._alpha_rl) + self._alpha_rl * choice_for_update * reward_for_update
    # Update H
    self.h = self.h * (1 - self._alpha_habit) + self._alpha_habit * choice_for_update
    
    self.current_index += 1

    # If the next trial is the beginning of a new session, reset beliefs 
    if self.current_index in session_index: 
      self.initialize_new_session()
  
  def reset_index(self):
    self.current_index = 0

class basic_ideal_observer_agent: #7 - Basic ideal observer 

  def __init__(
    self,
    beta_v, #model's tendency to select the port with the currently higher expected reward probability 
    beta_bias, 
    reward_prob_high = 0.8,
    reward_prob_low = 0.2,
    block_flip_prob = 0.02):

    self.beta_v = beta_v
    self.beta_bias = beta_bias
    self.reward_prob_high = reward_prob_high
    self.reward_prob_low = reward_prob_low
    self.block_flip_prob = block_flip_prob
    self.current_index = 0
    self.initialize_new_session()

  def initialize_new_session(self): # right and left state are initialized to the equal priors 
    self.belief_left_better = 0.5
  
  # policy 
  def get_choice(self):
    # Compute immediate expected reward
    Q_left = self.reward_prob_high * self.belief_left_better + self.reward_prob_low * (1-self.belief_left_better)
    Q_right = self.reward_prob_high * (1-self.belief_left_better) + self.reward_prob_low * self.belief_left_better

    # Compute choice (the most basic)
    logit = self.beta_v*(Q_right-Q_left) + self.beta_bias
    prob_choose_left = 1/(1+np.exp(-1*logit))
    choice = np.random.binomial(1,prob_choose_left)
    return choice, prob_choose_left 

  # basic ideal observer
  def update(self, choice, reward, session_index):

    # Convert choice and reward from 0/1 to -1/+1
    choice_for_update = 2*choice - 1  
    reward_for_update = 2*reward - 1
    
    # Check that the conversion worked
    assert choice_for_update == 1 or choice_for_update == -1
    assert reward_for_update == 1 or reward_for_update == -1

    ## Bayesian Update
    # Compute likelihood of the observation under each block
    if choice_for_update == -1: #left
      if reward_for_update == -1: #left no reward
        p_observation_given_left_better = self.reward_prob_low
        p_observation_given_right_better = self.reward_prob_high
      elif reward_for_update == 1: # left rewarded
        p_observation_given_left_better = self.reward_prob_high
        p_observation_given_right_better = self.reward_prob_low
    if choice_for_update == 1: # right
      if reward_for_update == -1: # right no reward
        p_observation_given_left_better = self.reward_prob_high
        p_observation_given_right_better = self.reward_prob_low 
      elif reward_for_update == 1:
        p_observation_given_left_better = self.reward_prob_low
        p_observation_given_right_better = self.reward_prob_high

    # Compute p(observation)
    p_observation = self.belief_left_better * p_observation_given_left_better + (1 - self.belief_left_better) * p_observation_given_right_better 

    # Bayes theorem update: Posterior = prior * likelihood
    belief_after_bayes_update = self.belief_left_better * p_observation_given_left_better / p_observation

    ## Dynamics Update
    belief_after_dynamics_update = belief_after_bayes_update * (1 - self.block_flip_prob) + (1 - belief_after_bayes_update) * self.block_flip_prob

    ## Set the belief property
    self.belief_left_better = belief_after_dynamics_update

    self.current_index += 1

    # If the next trial is the beginning of a new session, reset beliefs 
    if self.current_index in session_index: 
      self.initialize_new_session()
  
  def reset_index(self):
    self.current_index = 0

class ideal_observer_1_back_perserveration_agent: #8 - Beron et al., 2021

  def __init__(
    self,
    beta_v, #model's tendency to select the port with the currently higher expected reward probability 
    beta_bias,  
    beta_perserv, # 1-back perservation bias 
    reward_prob_high = 0.8,
    reward_prob_low = 0.2,
    block_flip_prob = 0.02):

    self.beta_v = beta_v
    self.beta_bias = beta_bias
    self.beta_perserv = beta_perserv
    self.reward_prob_high = reward_prob_high
    self.reward_prob_low = reward_prob_low
    self.block_flip_prob = block_flip_prob
    self.current_index = 0
    self.initialize_new_session()

  def initialize_new_session(self): # belief is initialized  
    self.belief_left_better = 0.5
    self.previous_choice = 0
  
  def get_choice(self):
    Q_left = self.reward_prob_high * self.belief_left_better + self.reward_prob_low * (1-self.belief_left_better)
    Q_right = self.reward_prob_high * (1-self.belief_left_better) + self.reward_prob_low * self.belief_left_better

    logit = self.beta_v*(Q_right-Q_left) + self.beta_bias + self.beta_perserv * self.previous_choice 
    prob_choose_left = 1/(1+np.exp(-1*logit))
    choice = np.random.binomial(1,prob_choose_left)
    return choice, prob_choose_left

  # update part is exactly same as the basic ideal observer agent 
  def update(self, choice, reward, session_index):

    # Convert choice and reward from 0/1 to -1/+1
    choice_for_update = 2*choice - 1  
    reward_for_update = 2*reward - 1

    self.previous_choice = choice_for_update
    
    # Check that the conversion worked
    assert choice_for_update == 1 or choice_for_update == -1
    assert reward_for_update == 1 or reward_for_update == -1

    ## Bayesian Update
    # Compute likelihood of the observation under each block
    if choice_for_update == -1: #left
      if reward_for_update == -1: #left no reward
        p_observation_given_left_better = self.reward_prob_low
        p_observation_given_right_better = self.reward_prob_high
      elif reward_for_update == 1: # left rewarded
        p_observation_given_left_better = self.reward_prob_high
        p_observation_given_right_better = self.reward_prob_low
    if choice_for_update == 1: # right
      if reward_for_update == -1: # right no reward
        p_observation_given_left_better = self.reward_prob_high
        p_observation_given_right_better = self.reward_prob_low 
      elif reward_for_update == 1:
        p_observation_given_left_better = self.reward_prob_low
        p_observation_given_right_better = self.reward_prob_high

    # Compute p(observation)
    p_observation = self.belief_left_better * p_observation_given_left_better + (1 - self.belief_left_better) * p_observation_given_right_better 

    # Bayes theorem update: Posterior = prior * likelihood
    belief_after_bayes_update = self.belief_left_better * p_observation_given_left_better / p_observation

    ## Dynamics Update
    belief_after_dynamics_update = belief_after_bayes_update * (1 - self.block_flip_prob) + (1 - belief_after_bayes_update) * self.block_flip_prob

    ## Set the belief property
    self.belief_left_better = belief_after_dynamics_update

    self.current_index += 1

    # If the next trial is the beginning of a new session, reset beliefs 
    if self.current_index in session_index: 
      self.initialize_new_session()

  def reset_index(self):
    self.current_index = 0

class ideal_observer_habits_agent: #9 - combination of basic ideal observer with habits agent from Miller et al., (2017)

  def __init__(
    self,
    beta_v,
    beta_habit,
    alpha_habit,
    beta_bias,
    reward_prob_high = 0.8,
    reward_prob_low = 0.2,
    block_flip_prob = 0.02):

    self.beta_v = beta_v
    self.alpha_habit = alpha_habit
    self.beta_habit = beta_habit
    self.beta_bias = beta_bias
    self.reward_prob_high = reward_prob_high
    self.reward_prob_low = reward_prob_low
    self.block_flip_prob = block_flip_prob
    self.current_index = 0
    self.initialize_new_session()

  def initialize_new_session(self): # right and left state are initialized to the equal priors 
    self.belief_left_better = 0.5
    self.h=0
  
  # policy 
  def get_choice(self):
    # Compute immediate expected reward
    Q_left = self.reward_prob_high * self.belief_left_better + self.reward_prob_low * (1-self.belief_left_better)
    Q_right = self.reward_prob_high * (1-self.belief_left_better) + self.reward_prob_low * self.belief_left_better

    # Compute choice with Habits RL 
    logit = self.beta_v * (Q_right-Q_left) + self.beta_habit * self.h + self.beta_bias
    prob_choose_left = 1/(1+np.exp(-1*logit))
    choice = np.random.binomial(1,prob_choose_left)
    return choice, prob_choose_left 

  # basic ideal observer
  def update(self, choice, reward, session_index):

    # Convert choice and reward from 0/1 to -1/+1
    choice_for_update = 2*choice - 1   
    reward_for_update = 2*reward - 1  
    
    # Check that the conversion worked
    assert choice_for_update == 1 or choice_for_update == -1
    assert reward_for_update == 1 or reward_for_update == -1

    ## Bayesian Update
    # Compute likelihood of the observation under each block
    if choice_for_update == -1: #left
      if reward_for_update == -1: #left no reward
        p_observation_given_left_better = self.reward_prob_low
        p_observation_given_right_better = self.reward_prob_high
      elif reward_for_update == 1: # left rewarded
        p_observation_given_left_better = self.reward_prob_high
        p_observation_given_right_better = self.reward_prob_low
    if choice_for_update == 1: # right
      if reward_for_update == -1: # right no reward
        p_observation_given_left_better = self.reward_prob_high
        p_observation_given_right_better = self.reward_prob_low 
      elif reward_for_update == 1:
        p_observation_given_left_better = self.reward_prob_low
        p_observation_given_right_better = self.reward_prob_high

    # Compute p(observation)
    p_observation = self.belief_left_better * p_observation_given_left_better + (1 - self.belief_left_better) * p_observation_given_right_better 

    # Bayes theorem update: Posterior = prior * likelihood
    belief_after_bayes_update = self.belief_left_better * p_observation_given_left_better / p_observation

    ## Dynamics Update
    belief_after_dynamics_update = belief_after_bayes_update * (1 - self.block_flip_prob) + (1 - belief_after_bayes_update) * self.block_flip_prob

    ## Set the belief property
    self.belief_left_better = belief_after_dynamics_update

    ## Update H 
    self.h = self.h * (1 - self.alpha_habit) + self.alpha_habit * choice_for_update 

    self.current_index += 1

    # If the next trial is the beginning of a new session, reset beliefs 
    if self.current_index in session_index: 
      self.initialize_new_session()
  
  def reset_index(self):
    self.current_index = 0
    
class marginal_value_theorem_agent: #10 - Constantino et al. (2015)

  def __init__(
      self,
      alpha_l,  #learning rate
      beta,  #scaling the decision variable (w) 
      beta_bias):
    self._alpha_l = alpha_l 
    self._beta = beta
    self._beta_bias = beta_bias
    self.general_rate_of_reward = 0  #V
    self.previous_choice = 0 
    self.current_index = 0
    self.local_reward_0 = 0 
    self.initialize_new_session()

  def initialize_new_session(self): 
    self.local_reward = 0.5 #L 
    self.previous_choice = 0 # left or right    
    
  # policy 
  def get_choice(self):  #here, decision_variable is all about switch and stay, so define the left and right is essential! 
    decision_variable = ((self.local_reward - self.general_rate_of_reward)/self._beta)+self._beta_bias 
    prob_choose_previous = 1/(1+np.exp(-2*decision_variable))
    if self.previous_choice == 0: #left - left 
      prob_choose_left = prob_choose_previous
    else: #previous choice was right 
      prob_choose_left = 1 - prob_choose_previous
    choice = np.random.binomial(1,prob_choose_left)
    return choice, prob_choose_left

  def update(self, choice, reward, session_index):
    new_local_reward = self._alpha_l*self.local_reward+(1-self._alpha_l)*reward
    if choice != self.previous_choice: 
      self.local_reward = self.local_reward_0
    else:
      self.local_reward = new_local_reward 

    self.previous_choice = choice
    self.current_index += 1

    # If the next trial is the beginning of a new session, reset beliefs 
    if self.current_index in session_index: 
      self.initialize_new_session()

  def reset_index(self):
    self.current_index = 0   